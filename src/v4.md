# Register reuse
[Full source][v4-rust]

In this version we are really starting to speed things up.
We will use a combination of ILP, SIMD, and loop unrolling to maximize CPU register usage in the hottest loop of the `step_row` function.
The Intel CPUs we are targeting have 16 [AVX registers][wiki-avx], each 256 bits wide, which match one-to-one with the `f32x8` type we have been using.
We'll use the same approach as in the [reference implementation][ppc-v4], which is to load 6 `f32x8` vectors from memory at each iteration and compute 9 results by combining all pairs.

[Here][ppc-v4-png] is a visualization that shows the big picture of what is happening.

First, we will group all rows of `vd` and `vt` into blocks of 3 rows.
Then, for every pair of 3-row blocks, we read 3+3 `f32x8`s and accumulate 9 different, intermediate `f32x8` results from the cartesian product of the vector pairs.
Finally, we extract values from the results accumulated in 9 `f32x8`s and write them to `r` in correct order.
The high-level idea is the same as in our other approaches: to do a bit of extra work outside the performance critical loop in order to do significantly less work inside the loop.


## Implementing `step_row_block`

Like in [`v2`][self-v2], we need to add some padding to make the amount of rows divisible by 3.
This time, however, we add the padding at the bottom of `vd` and `vt`, since the blocks are grouped vertically, by row.
Preprocessing is almost exactly the same as in [`v3`][self-v3], we pack all elements of `d` as `f32x8` vectors into `vd` and its transpose `vt`, except for the few extra rows at the bottom (unless the amount of rows is already divisible by 3):
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:init}}
```
Since we are processing rows in blocks of 3, it is probably easiest to also write results for 3 rows at a time.
Then we can chunk `vd` and `r` into 3-row blocks, zip them up, apply `step_row_block` in parallel such that each thread writes results for one block of 3 rows from `vd` into 3 rows of `r`.
Inside `step_row_block`, every thread will chunk `vt` into 3-row blocks, and computes results for every pair of `vt` row block `j` and `vd` row block `i`:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_block_head}}
```
Then, for every pair of row blocks `vd_row_block` and `vt_row_block`, we iterate over their columns, computing all 9 combinations of 3 `f32x8` vectors from `vd_row_block` and 3 `f32x8` vectors from `vt_row_block`, and add the results to the 9 intermediate results.
Before we go into the most performance-critical loop, we initialize 9 intermediate results to `f32x8` vectors (each containing 8 `f32::INFINITY`s), and extract all 6 rows from both row blocks:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_block_init}}
```
The reason we are not using a `tmp` array of 9 values is that the compiler was not keeping those 9 values in registers for the duration of the loop.

Now everything is set up for iterating column-wise, computing the usual "addition + minimum" between every element in `vt` and `vd`.
This time, we will load 6 `f32x8` vectors at each iteration, and compute 9 results in total.
We'll use the [`izip`-macro][rust-itertools-izip] from the `itertools` crate to get a nice, flattened tuple of row elements at each iteration:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_block_inner_loop}}
```

After we have iterated over all columns, we offset the block row indexes `i` and `j` so that we get a proper index mapping to the indexes of `r`, extract final results from all 9 intermediate results, and finally write them to `r`:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_block_results}}
```

## Full `step_row_block` implementation

```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_block}}
{{#include ../../shortcut-comparison/src/rust/v4_register_reuse/src/lib.rs:step_row_apply}}
```

## Benchmark

Let's run benchmarks with the same settings as before: `n = 6000`, single iteration, four threads bound to four cores.
C++ version available [here][v4-cpp].

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v4` | `gcc 7.4.0-1ubuntu1` | 4.2 | 2.26
C++ `v4` | `clang 6.0.0-1ubuntu2` | 3.7 | 1.92
Rust `v4` | `rustc 1.38.0-nightly` | 3.6 | 1.98

### `gcc`

```x86asm
{{#include asm/v4_cpp_gcc.asm}}
```
We see the expected output of 6 memory loads and 9+9 arithmetic instructions, but also quite a lot of register spilling in the middle and end of the loop.

It is unclear why the compiler decided to write intermediate results into memory already inside the loop, instead of keeping them in registers and doing the writing after the loop.
When compiling with `gcc 9.1.0`, these problems disappear.

### `clang`
```x86asm
{{#include asm/v4_cpp_clang.asm}}
```
This is a fairly clean and straightforward loop with almost nothing extra.
We load 6 SIMD vectors to 256-bit registers `ymm10-ymm15` and accumulate the results into 9 registers `ymm1-ymm9`, keeping `ymm0` as a temporary variable.
Notice how `rbx` is incremented by 32 bytes at each iteration, which is the size of a 256-bit SIMD vector.

### `rustc`
```x86asm
{{#include asm/v4_rs.asm}}
```
Same as `clang`s output, but instead of a loop counter that goes up, `r13` is decremented on each iteration.

{{#include LINKS.md}}
