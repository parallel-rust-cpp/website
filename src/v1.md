# Linear reading
[Full source][v1-rust]

To enable a linear memory access pattern, the [reference solution][ppc-v1] introduces a Θ(n²) preprocessing step that allocates additional space for storing the transpose of `d` in row-major order.
This allows us to read the columns of `d` linearly, using fully packed cache lines on each read.

The easiest way of allocating memory on the heap for contiguous elements is probably by creating a [vector][rust-vec-docs], which is a struct containing a pointer, size, and length.
We use the `std::vec` compile-time macro to create a mutable vector of length `n * n`, with all elements initialized to the value `0.0`, and then fill it with the transpose of `d`.
Note that there is no need to annotate the type of the vector, since `f32` is inferred from context:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/lib.rs:transpose}}
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/lib.rs:transpose_apply}}
```

Now all columns of `d` have been stored as rows in `t`, and all we have to do is to iterate over all row pair combinations of `d` and `t`.
As previously, we partition `r` into `n` non-overlapping, mutable rows such that each thread is working on one row at a time:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/bad_loop.rs:step_row}}
```

## Benchmark

We'll use the same settings as in [`v0`][self-v0].

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v1` | `gcc 7.4.0-1ubuntu1` | 60.5 | 1.54
C++ `v1` | `clang 6.0.0-1ubuntu2` | 60.5 | 1.00
Rust `v1` | `rustc 1.38.0-nightly` | 114.6 | 2.11

The linear memory access pattern helps a lot here, compared to what we had in the previous version.
However, the Rust program is struggling to keep up, executing twice the amount of instructions per cycle as the C++ program while being almost two times slower.
In the previous chapter, we talked about array bounds checking and `NaN` checks not affecting the running time due to a bad memory access pattern.
We fixed the memory access pattern but now the extra instructions are starting to slow us down.

Let's look at the most recent output from `rustc` to see these extra instructions.
This time, we skip `gcc` and `clang`, because they produced almost the same output as in [`v0`][self-v0].

### `rustc`

Not much has changed from [`v0`][self-v0], except that there is even more registers involved in doing bounds checking.
```x86asm
{{#include asm/v1_rs_bad.asm}}
```
Running the Rust program benchmark with [`perf-record`][perf-record-man] suggests that a significant amount of the running time is spent doing `NaN` checks with `vcmpunordss` and `vblendvps`.

### Dealing with the `NaN` check

Let's remove the `NaN` checks by replacing `f32::min` in the inner loop by a simple `if-else` expression:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/bad_loop.rs:step_row_inner_no_nan}}
```
Compiling and checking the output we see that the `NaN` checks are gone from our loop:
```x86asm
{{#include asm/v1_rs_bad_no_nan.asm}}
```

Benchmarking the Rust program shows that the running time also improved quite a lot:

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v1` | `gcc 7.4.0-1ubuntu1` | 60.5 | 1.54
C++ `v1` | `clang 6.0.0-1ubuntu2` | 60.5 | 1.00
Rust `v1` | `rustc 1.38.0-nightly` | 60.8 | 3.43

What about the array bounds checks?
Our mid-range CPU seems to be handling them without any problems even in the most performance critical loop.
However, the bounds checks are certainly not free, as we can see from the amount of IPC.
The C++ implementation of [`v1`][self-v1] is a proof that it is possible to solve the problem with significantly less instructions.
On other hand, we don't want to [remove the bounds checks][rust-get-unchecked-docs] completely, since we'd prefer to use as little `unsafe` Rust as possible.

### Dealing with the bounds checks

Our solution is similar to the preprocessing step of computing the transpose of `d`:
We will perform a bit of extra work outside the loop to remove a lot of work from inside the loop.
If we extract one row of `d` and one row of `t` as subslices before the inner loop starts, the compiler will have a chance to assert that the starting and ending index of the subslices are within the bounds of the slices we extract the subslices from:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/not_terrible_loop.rs:step_row}}
```
After compiling the program, we can see that the compiler still wants to check that `k` is in bounds.
Since `rsi` is incremented by 1 after each iteration, and it is used to load two `f32`s, it is very likely equal to our `k`.
```x86asm
{{#include asm/v1_rs_not_terrible.asm}}
```
Benchmarks show that the amount of IPC reduced significantly:

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v1` | `gcc 7.4.0-1ubuntu1` | 60.5 | 1.54
C++ `v1` | `clang 6.0.0-1ubuntu2` | 60.5 | 1.00
Rust `v1` | `rustc 1.38.0-nightly` | 60.6 | 2.02

Let's get all bounds checking out of the loop.
We are currently using `k` only for accessing every element of `d_row` and `t_row` between `0..n`, so we might as well use [iterators][rust-slice-iter-docs] over both subslices.
If we zip them them together, there's no need for `k` anymore.
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/not_terrible_loop.rs:step_row_inner_iter}}
```
After compiling the program, we can see that not only did the compiler remove the bounds checks but it also unrolled 8 iterations of the loop:
```x86asm
{{#include asm/v1_rs_slice_n_iter.asm}}
```
Recall how `clang` unrolled the loop in `v0` in an exactly similar way.
Since our program is still memory bottlenecked, the unrolling does not affect the running time.
However, it does reduce the total amount of IPC:

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v1` | `gcc 7.4.0-1ubuntu1` | 60.5 | 1.54
C++ `v1` | `clang 6.0.0-1ubuntu2` | 60.5 | 1.00
Rust `v1` | `rustc 1.38.0-nightly` | 60.6 | 0.92

The reason for this is that we have more instructions doing the useful stuff (e.g. loading memory `vmovss`, addition `vaddss`, and computing minimums `vminss`) than loop related instructions such as comparisons and jumps.
Compare this to the `gcc` single element loop of [`v0`][self-v0].


### `iter` all the things

If we succeeded in eliminating `k` from the innermost loop by using iterators, can we remove all loop variables with iterators?
We are using `chunks_mut` to divide `r` into rows of length `n`, so why not do something similar with `d` and `t` but with immutable chunks instead?

Our function computes `n` results for a row `i` in `d` into row `i` in `r`.
We can make `i` redundant by chunking `d` into rows at the same time as `r`, zip the row iterators into pairs and apply `step_row` in parallel on all `(r_row, d_row)` pairs.
Inside `step_row`, we loop over all columns `j` of `d`, i.e. all rows `j` of `t`.
If we chunk up `t` into `n` rows of length `n` inside `step_row`, we can zip up that iterator with row `i` of `r` and we have made index `j` redundant.

Finally, we wrap our `if-else` minimum into a function and put it into our toolbox:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/tools/src/lib.rs:min}}
```

Here's the final version of `v1` version of `step_row`:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/lib.rs:step_row}}
{{#include ../../shortcut-comparison/src/rust/v1_linear_reading/src/lib.rs:step_row_apply}}
```
Compiler output and benchmark results are not changed.

It's nice to see functional code that performs as well as a C++ program.
However, as we start pushing the CPU towards its limits, we eventually have to trade away some "functional prettiness" for raw performance, e.g. by loop unrolling and using hard-coded amounts of variables.

{{#include LINKS.md}}
