# SIMD
[Full source][v3-rust]

In this version we will be adding explicit [SIMD][wiki-simd] vector types and vector instructions to utilize CPU registers to their full width.
As we saw in [`v2`][self-v2], compilers are sometimes able to auto-vectorize simple loops.
This time, however, we will not be hoping for auto-vectorization magic, but we'll write all vector instructions directly into the code.
Since we only need a few simple instructions and are currently targeting only the `x86_64` platform, we won't be pulling in any external crates.
Instead, we define our own, tiny [`simd`-library][simd-tools-rust] with safe Rust wrappers around a few [Intel AVX intrinsics][intel-avx-guide].

We'll be using the same approach as in the [reference solution][v3-ppc], which is to pack all rows of `d` and `t` into 256-bit wide vectors (`f32x8`), each containing 8 single precision (`f32`) floats.
First, we initialize initialize two `std::vec::Vec` containers for `d` and its transpose `t`.
This time they will not contain `f32` values, but instead SIMD vectors of 8 `f32` elements:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v3_simd/src/lib.rs:init}}
```
We shouldn't have to worry about proper memory alignment since `std::vec::Vec` [by default][rust-vec-alloc] allocates its memory aligned to the size of the type of its elements.
Just to make sure, though, we added some debug asserts that check the alignment of each address in `vd` and `vt` by using this helper:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/tools/src/simd.rs:assert_aligned}}
```

Next, we will fill every row of `vd` and `vt` with `f32x8` vectors in parallel.
Each thread will read one row of `d` into `vd` and one column of `d` into `vt` in chunks of 8 elements.
We use two `f32` buffers of length 8, one for rows of `d` (`vx_tmp`) and one for columns of `d` (`vy_tmp`).
Each time the buffers become full, they are converted into two `f32x8` vectors and pushed to `vd` and `vt`:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v3_simd/src/lib.rs:preprocess}}
{{#include ../../shortcut-comparison/src/rust/v3_simd/src/lib.rs:preprocess_apply}}
```

The nice thing is that the preprocessing we just did is by far the hardest part.
Now all data is packed into SIMD vectors and we can use reuse `step_row` from [`v1`][self-v1] with minimal changes:
```rust,no_run,noplaypen
{{#include ../../shortcut-comparison/src/rust/v3_simd/src/lib.rs:step_row}}
{{#include ../../shortcut-comparison/src/rust/v3_simd/src/lib.rs:step_row_apply}}
```

## Benchmark

Let's run benchmarks with the same settings as in [`v2`][self-v2], comparing our Rust program to the reference [C++ version][v3-cpp].

Implementation | Compiler | Time (s) | IPC
:------|:---------|:---------|:---------------
C++ `v3` | `gcc 7.4.0-1ubuntu1` | 11.5 | 1.31
C++ `v3` | `clang 6.0.0-1ubuntu2` | 11.8 | 1.37
Rust `v3` | `rustc 1.38.0-nightly` | 11.4 | 1.04

The running times are roughly the same, but the Rust program clearly does less instructions per cycle compared to the C++ program.
Let's look at the disassembly to find out why.

### `gcc`
This is the single element loop from [`v0`][self-v0], but with 256-bit SIMD instructions and registers.
```x86asm
{{#include asm/v3_cpp_gcc.asm}}
```
More detailed analysis is available [here][ppc-v3-asm].

### `clang`
Like `gcc`, but for some reason there is a separate loop counter `r10`, instead of using `r9` both for loading values and checking if the loop has ended.
The extra addition could explain the higher instructions per cycle value.
```x86asm
{{#include asm/v3_cpp_clang.asm}}
```

### `rustc`

No bounds checking or extra instructions, except for a separate loop counter `r12`.
The loop has also been unrolled for 4 iterations, which is why we might be seeing the reduction in IPC.
```x86asm
{{#include asm/v3_rs.asm}}
```

{{#include LINKS.md}}
